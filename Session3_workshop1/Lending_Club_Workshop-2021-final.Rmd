---
title: "Data Science I, Workshop I: Predicting interest rates at the Lending Club"
author: "Group 5"
date: "12/10/2021"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r, load_libraries, include = FALSE, message=FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatterplot matrix
library(car) # vif() function to check for multicolinearity
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(here) # to read files and organise data
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(caret) # to train more advanced models (k-fold cross-validation, stepwise regression, LASSO)
library(zoo) #to allow for time series operations
library(glmnet)
library(here)

```


# Load and prepare the data

We start by loading the data to R in a dataframe.

```{r, load_data, warning=FALSE, message=FALSE}

lc_raw <- read_csv("LendingClub Data.csv",  skip=1) %>%  #since the first row is a title we want to skip it. 
  clean_names() # use janitor::clean_names()
```

# ICE the data: Inspect, Clean, Explore


```{r}

lc_clean<- lc_raw %>%
  dplyr::select(-x20:-x80) %>% #delete empty columns
  filter(!is.na(int_rate)) %>%   #delete empty rows
  mutate(
    issue_d = mdy(issue_d),  # lubridate::mdy() to fix date format
    term = factor(term_months),     # turn 'term' into a categorical variable
    delinq_2yrs = factor(delinq_2yrs) # turn 'delinq_2yrs' into a categorical variable
  ) %>% 
  dplyr::select(-emp_title,-installment, -term_months, everything()) #move some not-so-important variables to the end. 

```

The data is now in a clean format stored in the dataframe "lc_clean." 

## Q1 Exploratory data analysis and visualisation 


```{r, data_visualisation, message=FALSE}
# Build a histogram of interest rates.
ggplot(lc_clean, aes(x=int_rate))+  
  geom_histogram(binwidth=0.01)+
  scale_x_continuous(labels = scales::percent) +
  labs(x="Interest Rate", title = "Distribution of Interest Rates")

# Build a histogram of interest rates but use different color for loans of different grades 
ggplot(lc_clean, aes(x=int_rate, fill=grade))+  
  geom_histogram(binwidth=0.01)+scale_x_continuous(labels = scales::percent)+ 
  labs(x="Interest Rate", title = "Distribution of Interest Rates by Grade") 

# Produce a scatter plot of loan amount against interest rate and add visually the line of best fit
ggplot(lc_clean[seq(1, nrow(lc_clean), 10), ] , aes(y=int_rate, x=loan_amnt)) + 
  geom_point(size=0.1, alpha=0.5)+ 
  geom_smooth(method="lm", se=0) + 
  labs(y="Interest Rate", x="Loan Amount ($)", title = "Comparing Interest Rates and Loan Amounts")

# Produce a scatter plot of annual income against interest rate and add visually the line of best fit 
ggplot(lc_clean[seq(1, nrow(lc_clean), 10), ] , aes(y=int_rate, x=annual_inc)) + 
  geom_point(size=0.1)+ 
  geom_smooth(method="lm", se=0) +
  labs(y="Interest Rate", x="Annual Income ($)", title="Comparing Interest Rates and Annual Income")

# In the same axes, produce box plots of the interest rate for every value of delinquencies
ggplot(lc_clean , aes(y=int_rate, x=delinq_2yrs, colour= delinq_2yrs)) + 
  geom_boxplot()+
  # geom_jitter()+
  theme_bw()+
   scale_y_continuous(labels=scales::percent)+
  theme(legend.position = "none")+
  labs(
    title = "Do Delinquencies in the last Two Years Impact Interest Rates Charged?",
    x= "Number of Delinquecies in the last Two Years", y="Interest Rate"
  )


ggplot(lc_clean, aes (y=int_rate, x=issue_d, color = grade))+
  geom_point()+
  theme_bw()+
  labs(title="Interest Rate Evolution Over Time by Grade", x="Issue Date", y="Interest Rate")

lc_clean %>% 
  group_by (home_ownership) %>% 
  summarize (mean_int = mean(int_rate)) %>% 
  ggplot (aes(x = home_ownership, y= mean_int))+geom_col()+
  labs (title = "Relationship between Home Ownership and Interest Rates", x="Home Ownership Status", y="Interest Rate")

```

# Estimate simple linear regression models

We start with a simple but quite powerful model.

```{r, simple regression, message=FALSE}
# Use the lm command to estimate a regression model with the following variables "loan_amnt",  "term", "dti", "annual_inc", and "grade"

model1<-lm(int_rate ~ loan_amnt + term + dti + annual_inc + grade, data = lc_clean
  )
summary(model1)

```

## Q2 Analysis on model 1 (questions about model 1)

a. Are all variables statistically significant?

- All the values are statistically significant except for annual income. The P values for all variables are less than 0.05, but the P value for annual income is larder than 0.05. 

b. Interpret all the coefficients in the regression.

- For all the coefficients in this equation, its interpretation is that if the loan amount increases by 100,000 the interest rate will increase by 1.5%. 
- Additionally, if the loan term is 60 months,  the interest rate of the loan will be higher than that of a 36 month loan by on average 0.36%. Futhermore, a one unit increase to the debt-to-income ratio will increase on average the loan interest rate by 0.0043%. 
- Annual income is insignificant. Finally, comparing to Grade A, lowering the grade by each additional letter mean that the on average the interest rate will increase by 3.55% for Grabe B, 6.02% for Grade C, 8.17% for Grade D, 10% for Grade E, 12% for Grade F, and 14% for Grade G. 


c. How much explanatory power does the model have? 

- The model has an adjusted R squared of .9197. This means that the model can explain about 92% of the varibility of the interest rates with this data.

d. How wide would the 95% confidence interval of any prediction based on this model be? 

-  The confidence interval width is 4%, which is a high number when trying to predit interest rates. 

# Feature Engineering

Then we built progressively more complex models, with more features.


```{r, Feature Engineering, message=FALSE}

#Add to model 1 an interaction between loan amount and grade. Use the "var1*var2" notation to define an interaction term in the linear regression model. This will add the interaction and the individual variables to the model. 

model2 <- lm(int_rate ~ loan_amnt*grade + term +dti + annual_inc, data = lc_clean)
summary(model2)

#Add to the model we just created above the square and the cube of annual income. Use the poly(var_name,3) command as a variable in the linear regression model. 

model3 <-  lm(int_rate ~ loan_amnt*grade + term +dti + poly(annual_inc,3), data = lc_clean)
summary(model3)

#Continuing with the previous model, instead of annual income as a continuous variable break it down into quartiles and use quartile dummy variables. 
  
lc_clean <- lc_clean %>% 
  mutate(quartiles_annual_inc = as.factor(ntile(annual_inc, 4)))

model4 <-lm(int_rate ~ loan_amnt*grade + term +dti + quartiles_annual_inc, data = lc_clean)
summary(model4)  

#Compare the performance of these four models using the anova command
anova(model1, model2, model3, model4)


```

## Q3 Analysis on model 2-5 (questions about model 2-5)

a. Which of the four models has the most explanatory power in sample?

- When looking at the anova output, you can see that the R squared is increasing from each additional model, ending with an R-Squared of 92.04% for model 4, which we believe to have the most explanatory power.
- Also, when looking at the variables, model 4 has the most number of significant variables, while model 1,2 and 3 have a higher number of insignificant variables. This is partly due to the fact that the quartiles for annual income are more significant than annual income alone and the polynomial of annual income. 

b. In model 2, how can we interpret the estimated coefficient of the interaction term between grade B and loan amount? 

- The effect of increasing the loan amount is different for each grade. Using this interaction, we can see how the different Grades effect the interest rates taking into account the loan amount. 

c. The problem of multicollinearity describes situations in which one feature is correlated with other features (or with a linear combination of other features). If the goal is to use the model to make predictions, should we be concerned about multicollinearity? Why, or why not?

- When making predictions, multicollinearity is not an issue we need to worry about. All we care about is how good our prediction is, not how each variable effects the output. 

# Out of sample testing

We want to check the predictive accuracy of model2 by holding out a subset of the data to use as a testing data set. This method is sometimes referred to as the hold-out method for out-of-sample testing. 


```{r, out of sample testing}
# split the data in dataframe called "testing" and another one called  "training". The "training" dataframe should have 80% of the data and the "testing" dataframe 20%.

set.seed(1235)

train_test_split <- initial_split(lc_clean, prop = 0.80) # split the dataset into training (80% of data points) and testing (20% of data points) datasets
testing <- testing(train_test_split)
training <- training(train_test_split)

# Fit model2 on the training set 
model2_training<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt, training)

# Calculate the RMSE of the model in the training set (in sample)
rmse_training<-sqrt(mean((residuals(model2_training))^2))

print(rmse_training)

# Use the model to make predictions out of sample in the testing set
pred<-predict(model2_training,testing)
# Calculate the RMSE of the model in the testing set (out of sample)
rmse_testing<- RMSE(pred,testing$int_rate)

print(rmse_testing)

print(rmse_training - rmse_testing) # comparing the training and testing result of RMSE

```

## Q4 Predictive accuracy of Model 2 

- The predictive accuracy changes by .0001, which is basically zero. This is sensitive to the random seed chosen because RMSE changes when changeing the random seed. Finally, our RMSE is very similar in out training and testing set, giving us confidence that there is no evidence of overfitting. 


# k-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation. Using the caret package this is easy.

```{r, 10-fold cross validation}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)

```


```{r, 5-fold cross validation, eval=FALSE}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=5,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)

```

```{r, 15-fold cross validation, eval=FALSE}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=15,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)

```

## Q5 Comparison between 10-fold cross validation and hold-out method

- The RMSE of the hold out method for our training set was 0.01052 while the RMSE for the 10-fold cross validation is  0.01

- 10-fold cross is more reliable of a method as it uses all the data set as training and testing. The hold-out method reduces the data we use for training as it sets aside some data for the testing method. So, if we have a small data set, the data set we hold out can wrongly represent the whole data set. This gives us the option of using all the data points.  A drawback for 10-fold is the time it takes to run it K amount of times. 

- When running the 5-fold and 15-fold models, we get the same RMSE and R squared as the 10-fold. This means that the model is very robust. 


# Sample size estimation and learning curves

- We can use the hold out method for out-of-sample testing to check if we have a sufficiently large sample to estimate the model reliably. 
- The idea is to set aside some of the data as a testing set. From the remaining data draw progressively larger training sets and check how the performance of the model on the testing set changes. If the performance no longer improves with larger training sets we know we have a large enough sample.  The code below does this. Examine it and run it with different random seeds. 

```{r, learning curves}
#select a testing dataset (25% of all data)
set.seed(12)

train_test_split <- initial_split(lc_clean, prop = 0.75)
remaining <- training(train_test_split)
testing <- testing(train_test_split)

#We are now going to run 30 models starting from a tiny training set drawn from the training data and progressively increasing its size. The testing set remains the same in all iterations.

#initiating the model by setting some parameters to zero
rmse_sample <- 0
sample_size<-0
Rsq_sample<-0

for(i in 1:30) {
#from the remaining dataset select a smaller subset to training the data
set.seed(100)
sample

  learning_split <- initial_split(remaining, prop = i/200)
  training <- training(learning_split)
  sample_size[i]=nrow(training)
  
  #traing the model on the small dataset
  model3<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade + grade:loan_amnt, training)
  #test the performance of the model on the large testing dataset. This stays fixed for all iterations.
  pred<-predict(model3,testing)
  rmse_sample[i]<-RMSE(pred,testing$int_rate)
  Rsq_sample[i]<-R2(pred,testing$int_rate)
}
plot(sample_size,rmse_sample)
plot(sample_size,Rsq_sample)
```

## Q6 Analysis on learning curves

- When looking at the RMSE and R squared graphs, we can see that the RMSE decreases and the R squared increases a lot more before reaching the sample size of 2,000. 
- This means that we would need approximately 2,000 in our sample size in order to estimate model 3 reliably. Once we reach this sample size, we can reduce the prediction error by changing the variables in our model (feature engineering), but not increasing sample size. 


# Regularization using LASSO regression

If we are in the region of the learning curve where we do not have enough data, one option is to use a regularization method such as LASSO.

Let's try to estimate a large and complicated model (many interactions and polynomials) on a small training dataset using OLS regression and hold-out validation method.

```{r, OLS model overfitting}

#split the data in testing and training. The training test is really small.
set.seed(1234)
train_test_split <- initial_split(lc_clean, prop = 0.01)
training <- training(train_test_split)
testing <- testing(train_test_split)

model_lm<-lm(int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term, training)
predictions <- predict(model_lm,testing)

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)
```

Not surprisingly this model does not perform well -- as we knew form the learning curves we constructed for a simpler model we need a lot more data to estimate this model reliably. Try running it again with different seeds. The model's performance tends to be sensitive to the choice of the training set.

LASSO regression offers one solution -- it extends the OLS regression by penalizing the model for setting any coefficient estimate to a value that is different from zero. The penalty is proportional to a parameter lambda. This parameter cannot be estimated directly (and for this reason sometimes it is referred to as hyperparameter). lambda will be selected through k-fold cross validation so as to provide the best out-of-sample performance.  As a result of the LASSO procedure, only those features that are more strongly associated with the outcome will have non-zero coefficient estimates and the estimated model will be less sensitive to the training set. Sometimes LASSO regression is referred to as regularization. 

```{r, LASSO compared to OLS, warning=FALSE, message=FALSE}
# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas)
lambda_seq <- seq(0, 0.01, length = 1000)

# lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression. If alpha=0 the model would run ridge regression.
  )


# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)

# Best lambda
lasso$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero

sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

```

## Q7 Comparison between OLS regression and LASSO
a. Which model performs best out of sample, OLS regression or LASSO? Why?

- By comparing RMSE and LASSO, we can see LASSO performs better than OLS regression. The R square value increases by almost 0.03.

b. What value of lambda offers best performance? Is this sensitive to the random seed? Why?

- The model performs best with lambda equals to 0.00038. This number is sensitive to the random seed because the lamda changes with different random seeds.

c. How many coefficients are zero and how many are non-zero in the LASSO model of best fit? Is number of zero (or non-zero) coefficients sensitive on the random seed? Why?

- 26 coefficients are zero and 32 are non-zero. It is also sensitive to the random seed. We got different numbers with random seed of 4 and 5.

d. Why is it important to standardize continuous variables before running LASSO? 

-  Because we have a penalty on the absolute values of coefficients. With different scales of continuous variables, the levels of coefficients can be very different, which introduces imbanlance of importance of variables.

# Using Time Information

Let's try to further improve the model's predictive performance. So far we have not used any time series information. Effectively, all things being equal, our prediction for the interest rate of a loan given in 2009 would be the same as that of a loan given in 2011. We do not think it a good assumption and we are now taking into consideration the time series information to fix this in our previous models.
 
First, we investigate graphically whether there are any time trends in the interest rates. (Note that the variable "issue_d" only has information on the month the loan was awarded but not the exact date.) We want to use this to improve the model by tring controlling for time in a linear fashion (i.e., a linear time trend) and controlling for time as quarter-year dummies (this is a method to capture non-linear effects of time -- we assume that the impact of time doesn't change within a quarter but it can chance from quarter to quarter). Finally, we checked if time affect loans of different grades differently.

```{r, time trends}
#linear time trend (add code below)
ggplot(lc_clean, aes(x=issue_d, y=int_rate))+
  geom_point(size=0.1)+
  geom_smooth(method = "lm", se=FALSE)

#linear time trend by grade (add code below)
ggplot(lc_clean, aes(x=issue_d, y=int_rate, color=grade))+
  geom_point(size=0.1)+
  geom_smooth(method = "lm", se=FALSE)

#Train models using OLS regression and k-fold cross-validation
#The first model has some explanatory variables and a linear time trend

time1<-train(
  int_rate ~ loan_amnt*grade + term +dti + quartiles_annual_inc + issue_d , #fill your variables here "+ issue_d"
  lc_clean,
  method = "lm",
  trControl = control)

summary(time1)

#The second model has a different linear time trend for each grade class
time2<-train(
    int_rate ~ loan_amnt*grade + term +dti + quartiles_annual_inc + issue_d * grade, #fill your variables here 
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(time2)

#Change the time trend to a quarter dummy variables.
#zoo::as.yearqrt() creates quarter dummies 
lc_clean_quarter<-lc_clean %>%
  mutate(yq = as.factor(as.yearqtr(lc_clean$issue_d, format = "%Y-%m-%d")))



time3<-train(
    int_rate ~ loan_amnt*grade + term +dti + quartiles_annual_inc + yq,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )
  
summary(time3)

#We specify one quarter dummy variable for each grade. This is going to be a large model as there are 19 quarters x 7 grades = 133 quarter-grade dummies.
time4<-train(
    int_rate ~ loan_amnt*grade + term +dti + quartiles_annual_inc + yq * grade, #fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )

summary(time4)

data.frame(
  time1$results$RMSE,
  time2$results$RMSE,
  time3$results$RMSE,
  time4$results$RMSE)


```

## Q8 Prediction improvement by adding time information

- From the results of linear regressions, we find almost all the time related variables significant. Therefore, the interest rates do change over time. 
- The predictions can be improved by adding quarter-year dummies. The RMSE has been improved from 0.0103 to 0.0076.

# Using Bond Yields 
One concern with using time trends for forecasting is that in order to make predictions for future loans we will need to project trends to the future. This is an extrapolation that may not be reasonable, especially if macroeconomic conditions in the future change. Furthermore, if we are using quarter-year dummies, it is not even possible to estimate the coefficient of these dummy variables for future quarters.

Instead, perhaps it's better to find the reasons as to why different periods are different from one another. The csv file "MonthBondYields.csv" contains information on the yield of US Treasuries on the first day of each month. Can you use it to see if you can improve your predictions without using time dummies? 


```{r, bond yields}
#load the data to memory as a dataframe
bond_prices<-readr::read_csv("MonthBondYields.csv")

#make the date of the bond file comparable to the lending club dataset
#for some regional date/number (locale) settings this may not work. If it does try running the following line of code in the Console
#Sys.setlocale("LC_TIME","English")
bond_prices <- bond_prices %>%
  mutate(Date2=as.Date(paste("01",Date,sep="-"),"%d-%b-%y")) %>%
  select(-starts_with("X"))

#let's see what happened to bond yields over time. Lower bond yields mean the cost of borrowing has gone down.

bond_prices %>%
  ggplot(aes(x=Date2, y=Price))+
  geom_point(size=0.1, alpha=0.5)

#join the data using a left join
lc_with_bonds <- lc_clean %>%
  mutate(yq = as.factor(as.yearqtr(lc_clean$issue_d, format = "%Y-%m-%d"))) %>% 
  left_join(bond_prices, by = c("issue_d" = "Date2")) %>%
  arrange(issue_d) %>%
  filter(!is.na(Price)) #drop any observations where there re no bond prices available

# investigate graphically if there is a relationship 
lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price))+
  geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")

lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price, color=grade))+
  geom_point(size=0.1, alpha=0.5)+
  geom_smooth(method="lm")

#let's train a model using the bond information
control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE)

plsFit<-train(
    int_rate ~  loan_amnt*grade + term +dti + Price * grade, 
    lc_with_bonds,
    method = "lm",
    trControl = control
   )
summary(plsFit)

```


## Q9 Explanatory power of bond yields

* We are using loan amount times grade,term, dti and Price times grade to predict the interest rates
* Yes, by adding bond yields to the model, the adjusted R square increased by 0.01, and the bond yields are significant on every grade.
* With increase in bond yield (price) by 1%, the interest rates on the loan increase by 0.13% on Grade A loans. As the grade decreases (A to G) the effect of bond yields (price) on the interest rate starts to become negative. This means that as treasury  yields (price) increases, the effect on interest rates start to decrease.

## Q10 Model Comparison

```{r, Q10}
TestModel1<-train(
    int_rate ~  loan_amnt*grade + term +dti + Price * grade, 
    lc_with_bonds,
    method = "lm",
    trControl = control
   )
summary(TestModel1)

# TestModel2<-train(
#     int_rate ~  loan_amnt*grade + term +dti + Price * grade + quartiles_annual_inc + home_ownership + addr_state , 
#     lc_with_bonds,
#     method = "lm",
#     trControl = control
#    )
# summary(TestModel2)
# 
# TestModel3<-train(
#     int_rate ~  loan_amnt*grade + term +dti + Price * grade + quartiles_annual_inc + term*loan_amnt, 
#     lc_with_bonds,
#     method = "lm",
#     trControl = control
#    )
# summary(TestModel3)
```

- We have tried different models by adding home ownership, loan purpose, delinq_2 years,quartiles_annual_income and term$\times$loan_amt, and they have either decreased our R-squared or increased it very insignificantly. By adding more interaction terms and time information, the R^2 can be improved to 96% but it takes hours to run the model.

- After several attempts to increase the performance of the model (RMSE, R^2) by adding additional variables, we consider TestModel1 is the best model. This is because the performance doesn't improve significantly with more variables and TestModel1 is the least complex while with enough accuracy.

- TestModel1 uses loam_amnt$\times$grade, term, dti, Price$\times$grade, quartiles_annual_inc to predict the interest rate. It achieves an R^2 of 0.934 and an RMSE of 0.009584. The model can be used for prediction and extrapolated into the future as we didn't include any time dummy variables. 

- TestModel1 has a 95% confidence interval with a length of $1.96\times0.009584\times 2=0.0375$. 

## Q11 Further improvements 

Adding quarterly data on US [CPI](https://fred.stlouisfed.org/series/CPALTT01USQ657N):

```{r}
cpi <- readr::read_csv("ConsumerPriceIndex.csv") %>% 
  mutate(yq = as.factor(as.yearqtr(DATE, format = "%Y-%m-%d")))

lc_bonds_cpi <- lc_with_bonds %>% 
  left_join(cpi, by = c("yq")) %>% 
  arrange(issue_d) %>%
  filter(!is.na(CPALTT01USQ657N_NBD19600401)) #drop any observations where there re no bond prices available

# investigate graphically if there is a relationship 
lc_bonds_cpi%>%
  ggplot(aes(x=CPALTT01USQ657N_NBD19600401))+
  geom_histogram()

# investigate graphically if there is a relationship 
lc_bonds_cpi%>%
  ggplot(aes(x=int_rate, y=CPALTT01USQ657N_NBD19600401))+
  geom_point(size=0.1, alpha=0.5)+
  geom_smooth(method="lm")

lc_bonds_cpi%>%
  ggplot(aes(x=int_rate, y=CPALTT01USQ657N_NBD19600401, color=grade))+
  geom_point(size=0.1, alpha=0.5)+
  geom_smooth(method="lm")


plsFit<-train(
    int_rate ~  loan_amnt*grade + term +dti +  Price*grade+ CPALTT01USQ657N_NBD19600401 * grade, #fill your variables here 
    lc_bonds_cpi,
    method = "lm",
    trControl = control
   )
summary(plsFit)

```

## Analysis on additional data

- We would think that the additional data on inflation will make a difference because by adding both the bond yield and the inflation rate as variables to the model, it will give us a more accurate view on the real interest rate at the time. 
- However, when running the model, we can see that the adjusted r-squared only increases by more than 0.4% to 93.81%. The model also has a lower RMSE, making the predictions more accurate. 

# Team members
- Alex Kubbinga
- Clara Moreno Sanchez
- Jean Huang
- Raghav Mehta
- Raina Doshi
- Yuan Gao
